{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS ML APIs\n",
    "In this notebook, we walk you through setting up and using an AWS machine learning API via Sagemaker. The material in this notebook largely follows the [AWS Developer Guide](https://docs.aws.amazon.com/machine-learning/?id=docs_gateway) for each service while providing details which are specific to using them in an AWS Educate account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is machine learning?\n",
    "In QTM 220, you learned how to use linear models and their variants. The focus in that class was on the statistics of the models and their estimation. Another aspect of these models is their use for making predictions, a.k.a. inferences on new data. This is typically the focus of machine learning in practice, and linear models are one of the simplest and oldest machine learning models in use. In recent years, new models that are more complex than linear modesl have come to the forefront of machine learning practice as they can provide useful inferences when dealing with fundamental problems.\n",
    "\n",
    "The text in this subsection are selections from the [Amazon Machine Learning Developer Guide](https://docs.aws.amazon.com/machine-learning/latest/dg/machine-learning-problems-in-amazon-machine-learning.html) Caution: some of the services referenced in that particular guide are no longer available (e.g. Amazon ML). Hence, we present in this section a summary that is tailored to the background of  QTM 350 students.\n",
    "\n",
    "## Examples of contemporary business problems that ML is commonly used for\n",
    "\n",
    "Examples of binary classification problems:\n",
    "\n",
    "* Is this email spam or not spam? (e.g. Gmail spam filter)\n",
    "\n",
    "* Is this tweet written by a person or a robot? (E.g. Twitter [bot or not](https://blog.twitter.com/en_us/topics/company/2020/bot-or-not.html))\n",
    "\n",
    "Examples of multiclass classification problems:\n",
    "\n",
    "* Will this user want to watch a romantic comedy, documentary, or thriller? (E.g. Netflix [Recommendation algorithms](https://research.netflix.com/research-area/recommendations))\n",
    "\n",
    "* Which category of products is most interesting to this customer? (E.g. every online shopping site)\n",
    "\n",
    "Examples of regression classification problems:\n",
    "\n",
    "* How many days before this customer stops using the application? \n",
    "\n",
    "* What price will this house sell for? (e.g. Zillow [Zestimate](https://www.zillow.com/how-much-is-my-home-worth/))\n",
    "\n",
    "## When to use machine learning?\n",
    "It is important to remember that ML is not a solution for every type of problem. For example, you don’t need ML if you can determine a target value by using simple rules, computations, or predetermined steps that can be programmed without needing any data-driven learning.\n",
    "\n",
    "### Use machine learning for the following situations:\n",
    "\n",
    "#### You cannot code the rules\n",
    "Many human tasks (such as recognizing whether an email is spam or not spam) cannot be adequately solved using a simple (deterministic), rule-based solution. A large number of factors could influence the answer. When rules depend on too many factors and many of these rules overlap or need to be tuned very finely, it soon becomes difficult for a human to accurately code the rules. You can use ML to effectively solve this problem. \n",
    "\n",
    "#### You cannot scale\n",
    "You might be able to manually recognize a few hundred emails and decide whether they are spam or not. However, this task becomes tedious for millions of emails. ML solutions are effective at handling large-scale problems.\n",
    "\n",
    "### AI vrs ML\n",
    "Oftentimes simple rules or algorithms will be called AI, however they are not ML. See the section titled \"Process automation\" in this article [Harvard Business Review article](https://hbr.org/2018/01/artificial-intelligence-for-the-real-world). There, one of the tasks they list as being solved by AI is \"transferring data from e-mail and call center systems into systems of record—for example, updating customer files with address changes or service additions\". This requires programmatic control of data (for example by architecting a solution in the cloud to solve this task), not machine learning.\n",
    "\n",
    "In the same section we find, “reading” legal and contractual documents to extract provisions using natural language processing (NLP). This is ML. Indeed, NLP tasks cannot be solved with simple algorithms or programs and require the latest machine learning models.\n",
    "\n",
    "Talk to Jinho Choi at Emory if you are interested in NLP, as he is the faculty expert on campus and his team is the recent winner of the ultimate prize in this area, the [Alexa prize](https://developer.amazon.com/alexaprize)!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Rekognition\n",
    "We first look at a machine learning tool for image and video analysis. From the [documentation](https://docs.aws.amazon.com/rekognition/latest/dg/what-is.html) you will find that:\n",
    "> Amazon Rekognition is based on the same proven, highly scalable, deep learning technology developed by Amazon’s computer vision scientists to analyze billions of images and videos daily. It requires no machine learning expertise to use.\n",
    "\n",
    "Great! Let's get started.\n",
    "\n",
    "### Set up an IAM Role\n",
    "In order to use this API within Sagemaker, we will need to update the Role we have been using to control Sagemaker permissions. Recall, when you created your Sagemaker instance, one of the steps was creating a new IAM Role. If you used the suggested default, the name would be similar to AmazonSageMaker-ExecutionRole-0238127377.\n",
    "\n",
    "#### Where can I find that role?\n",
    "Go to your Sagemaker dashboard, then notebook instances, then click the notebook instance name to access the page for the \"Notebook instance settings\". You should then see the page below.\n",
    "\n",
    "![Notebook instance settings](./screenshot-instance-settings.png)\n",
    "\n",
    "\n",
    "There, under the heading \"Permissions and encryption\" click the link to the IAM role ARN. You should then see a view similar to this one below.\n",
    "\n",
    "\n",
    "![Notebook instance settings](./sagemaker-role.png)\n",
    "\n",
    "However, your view will have fewer policies, because I have already completed the step that you are about to complete, namely, adding policies to this Sagemaker role.\n",
    "\n",
    "#### Adding policies\n",
    "As we work with new AWS services within our notebooks, it will be necessary to add policies which give Sagemaker access to them. To use the examples we will present for working with Amazon Rekognition, you will need to add `AmazonRekognitionFullAccess` permissions. Also, `AmazonS3ReadOnlyAccess` is required for examples that access images or videos that are stored in an Amazon S3 bucket. Finally, the Amazon Rekognition Video stored video code examples also require `AmazonSQSFullAccess` permissions. \n",
    "\n",
    "To add them, in the IAM role Summary page (pictured in the last screenshot), click the blue \"Attach policies\" button. In the search bar, type the names of these services that were just listed, select them by ticking the empty white box next to the name when it appears, and then click the blue \"Attach policy\" button. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started using the console\n",
    "Before using the Rekognition service programatically, it will be helpful to understand what it does by walking through examples in the AWS console. To do that, complete Exercises 1 through 4 listed [here](https://docs.aws.amazon.com/rekognition/latest/dg/getting-started-console.html), then return to this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Rekognition API programatically\n",
    "In this section, you use the Amazon Rekognition Image API operations to analyze images stored in an Amazon S3 bucket.\n",
    "\n",
    "#### Step 1\n",
    "Create a new S3 bucket. This can be done graphically via the AWS console, or programaticaly using bash or the Python SDK.\n",
    "\n",
    "### Set up the AWS CLI\n",
    "In any new Sagemaker instance, the AWS CLI (Command Line Interface) comes preinstalled. Indeed, to check that this is the case, run the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3()                                                                      S3()\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mNAME\u001b[0m\n",
      "       s3 -\n",
      "\n",
      "\u001b[1mDESCRIPTION\u001b[0m\n",
      "       This  section  explains  prominent concepts and notations in the set of\n",
      "       high-level S3 commands provided.\n",
      "\n",
      "   \u001b[1mPath Argument Type\u001b[0m\n",
      "       Whenever using a command, at least one path argument must be specified.\n",
      "       There are two types of path arguments: \u001b[1mLocalPath \u001b[22mand \u001b[1mS3Uri\u001b[22m.\n",
      "\n",
      "       \u001b[1mLocalPath\u001b[22m: represents the path of a local file or directory.  It can be\n",
      "       written as an absolute path or relative path.\n",
      "\n",
      "       \u001b[1mS3Uri\u001b[22m: represents the location of a S3 object, prefix, or bucket.  This\n",
      "       must  be  written in the form \u001b[1ms3://mybucket/mykey \u001b[22mwhere \u001b[1mmybucket \u001b[22mis the\n",
      "       specified S3 bucket, \u001b[1mmykey \u001b[22mis the specified S3 key.  The path  argument\n",
      "       must  begin with \u001b[1ms3:// \u001b[22min order to denote that the path argument refers\n",
      "       to a S3 object. Note that prefixes are separated  by  forward  slashes.\n",
      "       For  example, if the S3 object \u001b[1mmyobject \u001b[22mhad the prefix \u001b[1mmyprefix\u001b[22m, the S3\n",
      "       key would be \u001b[1mmyprefix/myobject\u001b[22m, and if the object  was  in  the  bucket\n",
      "       \u001b[1mmybucket\u001b[22m, the \u001b[1mS3Uri \u001b[22mwould be \u001b[1ms3://mybucket/myprefix/myobject\u001b[22m.\n",
      "\n",
      "       \u001b[1mS3Uri  \u001b[22malso supports S3 access points. To specify an access point, this\n",
      "       value must be of the form \u001b[1ms3://<access-point-arn>/<key>\u001b[22m. For example if\n",
      "       the   access   point   \u001b[1mmyaccesspoint   \u001b[22mto   be   used   has   the  ARN:\n",
      "       \u001b[1marn:aws:s3:us-west-2:123456789012:accesspoint/myaccesspoint   \u001b[22mand   the\n",
      "       object  being  accessed has the key \u001b[1mmykey\u001b[22m, then the \u001b[1mS3URI \u001b[22mused must be:\n",
      "       \u001b[1ms3://arn:aws:s3:us-west-2:123456789012:accesspoint/myaccesspoint/mykey\u001b[22m.\n",
      "       Similar  to  bucket  names, you can also use prefixes with access point\n",
      "       ARNs        for         the         \u001b[1mS3Uri\u001b[22m.         For         example:\n",
      "       \u001b[1ms3://arn:aws:s3:us-west-2:123456789012:accesspoint/myaccesspoint/mypre-\u001b[0m\n",
      "       \u001b[1mfix/\u001b[0m\n",
      "\n",
      "       The higher level \u001b[1ms3 \u001b[22mcommands do \u001b[1mnot \u001b[22msupport access point  object  ARNs.\n",
      "       For      example,      if      the     following     was     specified:\n",
      "       \u001b[1ms3://arn:aws:s3:us-west-2:123456789012:accesspoint/myaccess-\u001b[0m\n",
      "       \u001b[1mpoint/object/mykey   \u001b[22mthe   \u001b[1mS3URI   \u001b[22mwill   resolve  to  the  object  key\n",
      "       \u001b[1mobject/mykey\u001b[0m\n",
      "\n",
      "   \u001b[1mOrder of Path Arguments\u001b[0m\n",
      "       Every command takes one or two positional path  arguments.   The  first\n",
      "       path  argument represents the source, which is the local file/directory\n",
      "       or S3 object/prefix/bucket that is being referenced.   If  there  is  a\n",
      "       second path argument, it represents the destination, which is the local\n",
      "       file/directory or S3 object/prefix/bucket that is  being  operated  on.\n",
      "       Commands  with only one path argument do not have a destination because\n",
      "       the operation is being performed only on the source.\n",
      "\n",
      "   \u001b[1mSingle Local File and S3 Object Operations\u001b[0m\n",
      "       Some commands perform operations only on single files and  S3  objects.\n",
      "       The following commands are single file/object operations if no \u001b[1m--recur-\u001b[0m\n",
      "       \u001b[1msive \u001b[22mflag is provided.\n",
      "\n",
      "          o \u001b[1mcp\u001b[0m\n",
      "\n",
      "          o \u001b[1mmv\u001b[0m\n",
      "\n",
      "          o \u001b[1mrm\u001b[0m\n",
      "\n",
      "       For this type of operation, the first path argument, the  source,  must\n",
      "       exist  and be a local file or S3 object.  The second path argument, the\n",
      "       destination, can be the name of  a  local  file,  local  directory,  S3\n",
      "       object, S3 prefix, or S3 bucket.\n",
      "\n",
      "       The  destination  is  indicated  as a local directory, S3 prefix, or S3\n",
      "       bucket if it ends with a forward slash or back slash.  The use of slash\n",
      "       depends  on  the  path argument type.  If the path argument is a \u001b[1mLocal-\u001b[0m\n",
      "       \u001b[1mPath\u001b[22m, the type of slash is the separator used by the operating  system.\n",
      "       If  the  path  is a \u001b[1mS3Uri\u001b[22m, the forward slash must always be used.  If a\n",
      "       slash is at the end of the destination, the destination file or  object\n",
      "       will  adopt the name of the source file or object.  Otherwise, if there\n",
      "       is no slash at the end, the file or object will be saved under the name\n",
      "       provided.  See examples in \u001b[1mcp \u001b[22mand \u001b[1mmv \u001b[22mto illustrate this description.\n",
      "\n",
      "   \u001b[1mDirectory and S3 Prefix Operations\u001b[0m\n",
      "       Some commands only perform operations on the contents of a local direc-\n",
      "       tory or S3 prefix/bucket.  Adding or omitting a forward slash  or  back\n",
      "       slash  to the end of any path argument, depending on its type, does not\n",
      "       affect the results of  the  operation.   The  following  commands  will\n",
      "       always result in a directory or S3 prefix/bucket operation:\n",
      "\n",
      "       o \u001b[1msync\u001b[0m\n",
      "\n",
      "       o \u001b[1mmb\u001b[0m\n",
      "\n",
      "       o \u001b[1mrb\u001b[0m\n",
      "\n",
      "       o \u001b[1mls\u001b[0m\n",
      "\n",
      "   \u001b[1mUse of Exclude and Include Filters\u001b[0m\n",
      "       Currently, there is no support for the use of UNIX style wildcards in a\n",
      "       command's  path  arguments.   However,  most  commands  have  \u001b[1m--exclude\u001b[0m\n",
      "       \u001b[1m\"<value>\"  \u001b[22mand  \u001b[1m--include  \"<value>\"  \u001b[22mparameters  that  can achieve the\n",
      "       desired result.  These parameters perform pattern  matching  to  either\n",
      "       exclude  or include a particular file or object.  The following pattern\n",
      "       symbols are supported.\n",
      "\n",
      "          o \u001b[1m*\u001b[22m: Matches everything\n",
      "\n",
      "          o \u001b[1m?\u001b[22m: Matches any single character\n",
      "\n",
      "          o \u001b[1m[sequence]\u001b[22m: Matches any character in \u001b[1msequence\u001b[0m\n",
      "\n",
      "          o \u001b[1m[!sequence]\u001b[22m: Matches any character not in \u001b[1msequence\u001b[0m\n",
      "\n",
      "       Any number of these parameters can be passed to a command.  You can  do\n",
      "       this  by  providing  an \u001b[1m--exclude \u001b[22mor \u001b[1m--include \u001b[22margument multiple times,\n",
      "       e.g.  \u001b[1m--include \"*.txt\" --include \"*.png\"\u001b[22m.   When  there  are  multiple\n",
      "       filters,  the rule is the filters that appear later in the command take\n",
      "       precedence over filters that appear earlier in the command.  For  exam-\n",
      "       ple, if the filter parameters passed to the command were\n",
      "\n",
      "          --exclude \"*\" --include \"*.txt\"\n",
      "\n",
      "       All  files  will  be  excluded from the command except for files ending\n",
      "       with \u001b[1m.txt  \u001b[22mHowever, if the order of the filter parameters  was  changed\n",
      "       to\n",
      "\n",
      "          --include \"*.txt\" --exclude \"*\"\n",
      "\n",
      "       All files will be excluded from the command.\n",
      "\n",
      "       Each  filter  is evaluated against the \u001b[1msource directory\u001b[22m.  If the source\n",
      "       location is a file instead of a directory, the directory containing the\n",
      "       file is used as the source directory.  For example, suppose you had the\n",
      "       following directory structure:\n",
      "\n",
      "          /tmp/foo/\n",
      "            .git/\n",
      "            |---config\n",
      "            |---description\n",
      "            foo.txt\n",
      "            bar.txt\n",
      "            baz.jpg\n",
      "\n",
      "       In the command \u001b[1maws s3 sync /tmp/foo s3://bucket/ \u001b[22mthe  source  directory\n",
      "       is  \u001b[1m/tmp/foo\u001b[22m.   Any  include/exclude filters will be evaluated with the\n",
      "       source directory prepended.  Below are several examples to  demonstrate\n",
      "       this.\n",
      "\n",
      "       Given  the directory structure above and the command \u001b[1maws s3 cp /tmp/foo\u001b[0m\n",
      "       \u001b[1ms3://bucket/ --recursive --exclude \".git/*\"\u001b[22m, the files \u001b[1m.git/config  \u001b[22mand\n",
      "       \u001b[1m.git/description  \u001b[22mwill be excluded from the files to upload because the\n",
      "       exclude filter \u001b[1m.git/* \u001b[22mwill have the source  prepended  to  the  filter.\n",
      "       This means that:\n",
      "\n",
      "          /tmp/foo/.git/* -> /tmp/foo/.git/config       (matches, should exclude)\n",
      "          /tmp/foo/.git/* -> /tmp/foo/.git/description  (matches, should exclude)\n",
      "          /tmp/foo/.git/* -> /tmp/foo/foo.txt  (does not match, should include)\n",
      "          /tmp/foo/.git/* -> /tmp/foo/bar.txt  (does not match, should include)\n",
      "          /tmp/foo/.git/* -> /tmp/foo/baz.jpg  (does not match, should include)\n",
      "\n",
      "       The  command  \u001b[1maws  s3  cp  /tmp/foo/ s3://bucket/ --recursive --exclude\u001b[0m\n",
      "       \u001b[1m\"ba*\" \u001b[22mwill exclude \u001b[1m/tmp/foo/bar.txt \u001b[22mand \u001b[1m/tmp/foo/baz.jpg\u001b[22m:\n",
      "\n",
      "          /tmp/foo/ba* -> /tmp/foo/.git/config      (does not match, should include)\n",
      "          /tmp/foo/ba* -> /tmp/foo/.git/description (does not match, should include)\n",
      "          /tmp/foo/ba* -> /tmp/foo/foo.txt          (does not match, should include)\n",
      "          /tmp/foo/ba* -> /tmp/foo/bar.txt  (matches, should exclude)\n",
      "          /tmp/foo/ba* -> /tmp/foo/baz.jpg  (matches, should exclude)\n",
      "\n",
      "       Note that, by default, \u001b[4mall\u001b[24m \u001b[4mfiles\u001b[24m \u001b[4mare\u001b[24m \u001b[4mincluded\u001b[24m.  This means that provid-\n",
      "       ing  \u001b[1monly  \u001b[22man  \u001b[1m--include  \u001b[22mfilter  will not change what files are trans-\n",
      "       ferred.  \u001b[1m--include \u001b[22mwill only re-include files that have  been  excluded\n",
      "       from an \u001b[1m--exclude \u001b[22mfilter.  If you only want to upload files with a par-\n",
      "       ticular extension, you need to first exclude all files, then re-include\n",
      "       the files with the particular extension.  This command will upload \u001b[1monly\u001b[0m\n",
      "       files ending with \u001b[1m.jpg\u001b[22m:\n",
      "\n",
      "          aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \"*\" --include \"*.jpg\"\n",
      "\n",
      "       If you wanted to include both \u001b[1m.jpg \u001b[22mfiles as well as \u001b[1m.txt \u001b[22mfiles you  can\n",
      "       run:\n",
      "\n",
      "          aws s3 cp /tmp/foo/ s3://bucket/ --recursive \\\n",
      "              --exclude \"*\" --include \"*.jpg\" --include \"*.txt\"\n",
      "\n",
      "       See 'aws help' for descriptions of global parameters.\n",
      "\n",
      "\u001b[1mSYNOPSIS\u001b[0m\n",
      "          aws s3 <Command> [<Arg> ...]\n",
      "\n",
      "\u001b[1mOPTIONS\u001b[0m\n",
      "       \u001b[4mNone\u001b[0m\n",
      "\n",
      "       See 'aws help' for descriptions of global parameters.\n",
      "\n",
      "\u001b[1mAVAILABLE COMMANDS\u001b[0m\n",
      "       o cp\n",
      "\n",
      "       o ls\n",
      "\n",
      "       o mb\n",
      "\n",
      "       o mv\n",
      "\n",
      "       o presign\n",
      "\n",
      "       o rb\n",
      "\n",
      "       o rm\n",
      "\n",
      "       o sync\n",
      "\n",
      "       o website\n",
      "\n",
      "\n",
      "\n",
      "                                                                          S3()\n"
     ]
    }
   ],
   "source": [
    "!aws s3 help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see after running the above cell a list of commands that are available for working with S3 using the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the command `mb` below to make a new bucket. As bucket names must be globally unique, you may need to modify the name slightly, for example by adding your name to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket: image-api-example\n"
     ]
    }
   ],
   "source": [
    " !aws s3 mb s3://image-api-example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the `aws s3 ls` command to list buckets in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-15 11:14:16 image-api-example\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to add an image to this bucket. There are many ways to do that. We will continue using the aws cli in order to accomplish this.\n",
    "\n",
    "First, using the JupyterLab file viewer on the left, click the up arrow symbol to upload a file to your Sagemaker instance. Upload either a .jpg or a .png. For example, I uploaded my profile picture named `jeremyjacobson.png`. Now move this file to your bucket using the `aws s3 mv` command. It works just like the `mv` command in Linux. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "move: ./jeremyjacobson.png to s3://image-api-example/jeremyjacobson.png\n"
     ]
    }
   ],
   "source": [
    "!aws s3 mv jeremyjacobson.png s3://image-api-example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having run the cell above, the file should no longer appear in the JupyterLab file viewer. Let's check what is in the bucket we made now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-15 11:19:22     127954 jeremyjacobson.png\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls image-api-example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, we see that the image is in the bucket. Let's move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detect labels in an image\n",
    "This example displays the JSON output from the `detect-labels` call to  the Rekognition API. You will need to modify it by replacing Bucket with your bucket's name and replace Name with your file's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Labels\": [\n",
      "        {\n",
      "            \"Name\": \"Human\",\n",
      "            \"Confidence\": 98.61043548583984,\n",
      "            \"Instances\": [],\n",
      "            \"Parents\": []\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Person\",\n",
      "            \"Confidence\": 98.61043548583984,\n",
      "            \"Instances\": [\n",
      "                {\n",
      "                    \"BoundingBox\": {\n",
      "                        \"Width\": 0.8968233466148376,\n",
      "                        \"Height\": 0.8738352060317993,\n",
      "                        \"Left\": 0.07586783170700073,\n",
      "                        \"Top\": 0.11023861169815063\n",
      "                    },\n",
      "                    \"Confidence\": 98.61043548583984\n",
      "                }\n",
      "            ],\n",
      "            \"Parents\": []\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Face\",\n",
      "            \"Confidence\": 97.62527465820312,\n",
      "            \"Instances\": [],\n",
      "            \"Parents\": [\n",
      "                {\n",
      "                    \"Name\": \"Person\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Smile\",\n",
      "            \"Confidence\": 85.75872039794922,\n",
      "            \"Instances\": [],\n",
      "            \"Parents\": [\n",
      "                {\n",
      "                    \"Name\": \"Face\"\n",
      "                },\n",
      "                {\n",
      "                    \"Name\": \"Person\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Man\",\n",
      "            \"Confidence\": 83.50458526611328,\n",
      "            \"Instances\": [],\n",
      "            \"Parents\": [\n",
      "                {\n",
      "                    \"Name\": \"Person\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Clothing\",\n",
      "            \"Confidence\": 73.89022827148438,\n",
      "            \"Instances\": [],\n",
      "            \"Parents\": []\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Apparel\",\n",
      "            \"Confidence\": 73.89022827148438,\n",
      "            \"Instances\": [],\n",
      "            \"Parents\": []\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Shirt\",\n",
      "            \"Confidence\": 68.68953704833984,\n",
      "            \"Instances\": [],\n",
      "            \"Parents\": [\n",
      "                {\n",
      "                    \"Name\": \"Clothing\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Dating\",\n",
      "            \"Confidence\": 60.82618713378906,\n",
      "            \"Instances\": [],\n",
      "            \"Parents\": [\n",
      "                {\n",
      "                    \"Name\": \"Person\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Photography\",\n",
      "            \"Confidence\": 57.578369140625,\n",
      "            \"Instances\": [],\n",
      "            \"Parents\": [\n",
      "                {\n",
      "                    \"Name\": \"Person\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Photo\",\n",
      "            \"Confidence\": 57.578369140625,\n",
      "            \"Instances\": [],\n",
      "            \"Parents\": [\n",
      "                {\n",
      "                    \"Name\": \"Person\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Student\",\n",
      "            \"Confidence\": 57.21558380126953,\n",
      "            \"Instances\": [],\n",
      "            \"Parents\": [\n",
      "                {\n",
      "                    \"Name\": \"Person\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Dimples\",\n",
      "            \"Confidence\": 56.37268829345703,\n",
      "            \"Instances\": [],\n",
      "            \"Parents\": [\n",
      "                {\n",
      "                    \"Name\": \"Face\"\n",
      "                },\n",
      "                {\n",
      "                    \"Name\": \"Person\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Portrait\",\n",
      "            \"Confidence\": 55.229400634765625,\n",
      "            \"Instances\": [],\n",
      "            \"Parents\": [\n",
      "                {\n",
      "                    \"Name\": \"Face\"\n",
      "                },\n",
      "                {\n",
      "                    \"Name\": \"Photography\"\n",
      "                },\n",
      "                {\n",
      "                    \"Name\": \"Person\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"LabelModelVersion\": \"2.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws rekognition detect-labels --image '{\"S3Object\":{\"Bucket\":\"image-api-example\", \"Name\":\"jeremyjacobson.png\"}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step could be to use a command line tool like `jq` to extract from this JSON the information we are most interested in and use that to create a new dataset as we did in earlier notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Python SDK\n",
    "We start by importing the package which containts the code for the Python SDK, `boto3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#PDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-rekognition-developer-guide/blob/master/LICENSE-SAMPLECODE.)\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an instance `client` of the client object in the `boto3` package for `rekognition`. It will allow use to communicate and make requests to the Rekognition service using Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=boto3.client('rekognition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the client, we use the dot notation to access one of its methods, `detect_labels`. This example displays the labels that were detected in the input image like we did previously using the CLI. Again, replace the values of bucket and photo with the names of the Amazon S3 bucket and image that you used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.detect_labels(Image={'S3Object':{'Bucket':\"image-api-example\",'Name':\"jeremyjacobson.png\"}}, MaxLabels=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate what we have obtained as a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it is a Python dictionary. Let's see what are the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Labels', 'LabelModelVersion', 'ResponseMetadata'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the values for the `Labels` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'Person',\n",
       "  'Confidence': 98.61043548583984,\n",
       "  'Instances': [{'BoundingBox': {'Width': 0.8968233466148376,\n",
       "     'Height': 0.8738352060317993,\n",
       "     'Left': 0.07586783170700073,\n",
       "     'Top': 0.11023861169815063},\n",
       "    'Confidence': 98.61043548583984}],\n",
       "  'Parents': []},\n",
       " {'Name': 'Face',\n",
       "  'Confidence': 97.62527465820312,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Person'}]},\n",
       " {'Name': 'Smile',\n",
       "  'Confidence': 85.75872039794922,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Face'}, {'Name': 'Person'}]},\n",
       " {'Name': 'Man',\n",
       "  'Confidence': 83.50458526611328,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Person'}]},\n",
       " {'Name': 'Clothing',\n",
       "  'Confidence': 73.89022827148438,\n",
       "  'Instances': [],\n",
       "  'Parents': []},\n",
       " {'Name': 'Shirt',\n",
       "  'Confidence': 68.68953704833984,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Clothing'}]},\n",
       " {'Name': 'Dating',\n",
       "  'Confidence': 60.82618713378906,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Person'}]},\n",
       " {'Name': 'Photography',\n",
       "  'Confidence': 57.578369140625,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Person'}]},\n",
       " {'Name': 'Student',\n",
       "  'Confidence': 57.21558380126953,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Person'}]},\n",
       " {'Name': 'Dimples',\n",
       "  'Confidence': 56.37268829345703,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Face'}, {'Name': 'Person'}]}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['Labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see it is the same data as obtained before. Now, using Python and Pandas we could extract the data we want from this request and use it to create a new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting faces in an image\n",
    "Here is another example. See the [documentation](https://docs.aws.amazon.com/rekognition/latest/dg/faces-detect-images.html) for details on this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"FaceDetails\": [\n",
      "        {\n",
      "            \"BoundingBox\": {\n",
      "                \"Width\": 0.35594138503074646,\n",
      "                \"Height\": 0.47563669085502625,\n",
      "                \"Left\": 0.28262796998023987,\n",
      "                \"Top\": 0.22201739251613617\n",
      "            },\n",
      "            \"AgeRange\": {\n",
      "                \"Low\": 33,\n",
      "                \"High\": 49\n",
      "            },\n",
      "            \"Smile\": {\n",
      "                \"Value\": true,\n",
      "                \"Confidence\": 98.935546875\n",
      "            },\n",
      "            \"Eyeglasses\": {\n",
      "                \"Value\": true,\n",
      "                \"Confidence\": 99.30152893066406\n",
      "            },\n",
      "            \"Sunglasses\": {\n",
      "                \"Value\": false,\n",
      "                \"Confidence\": 91.17986297607422\n",
      "            },\n",
      "            \"Gender\": {\n",
      "                \"Value\": \"Male\",\n",
      "                \"Confidence\": 93.72130584716797\n",
      "            },\n",
      "            \"Beard\": {\n",
      "                \"Value\": false,\n",
      "                \"Confidence\": 88.77381896972656\n",
      "            },\n",
      "            \"Mustache\": {\n",
      "                \"Value\": false,\n",
      "                \"Confidence\": 98.5105972290039\n",
      "            },\n",
      "            \"EyesOpen\": {\n",
      "                \"Value\": true,\n",
      "                \"Confidence\": 99.9191665649414\n",
      "            },\n",
      "            \"MouthOpen\": {\n",
      "                \"Value\": true,\n",
      "                \"Confidence\": 98.58784484863281\n",
      "            },\n",
      "            \"Emotions\": [\n",
      "                {\n",
      "                    \"Type\": \"HAPPY\",\n",
      "                    \"Confidence\": 99.3108139038086\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"SURPRISED\",\n",
      "                    \"Confidence\": 0.150288924574852\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"CONFUSED\",\n",
      "                    \"Confidence\": 0.1296592503786087\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"FEAR\",\n",
      "                    \"Confidence\": 0.10119867324829102\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"DISGUSTED\",\n",
      "                    \"Confidence\": 0.08960969746112823\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"ANGRY\",\n",
      "                    \"Confidence\": 0.08061101287603378\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"CALM\",\n",
      "                    \"Confidence\": 0.07646901905536652\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"SAD\",\n",
      "                    \"Confidence\": 0.0613456591963768\n",
      "                }\n",
      "            ],\n",
      "            \"Landmarks\": [\n",
      "                {\n",
      "                    \"Type\": \"eyeLeft\",\n",
      "                    \"X\": 0.37144890427589417,\n",
      "                    \"Y\": 0.4370081126689911\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"eyeRight\",\n",
      "                    \"X\": 0.5332980751991272,\n",
      "                    \"Y\": 0.4361833930015564\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"mouthLeft\",\n",
      "                    \"X\": 0.3862433135509491,\n",
      "                    \"Y\": 0.5987624526023865\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"mouthRight\",\n",
      "                    \"X\": 0.5209956169128418,\n",
      "                    \"Y\": 0.5981724858283997\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"nose\",\n",
      "                    \"X\": 0.4387911856174469,\n",
      "                    \"Y\": 0.5176153779029846\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"leftEyeBrowLeft\",\n",
      "                    \"X\": 0.31642478704452515,\n",
      "                    \"Y\": 0.39972352981567383\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"leftEyeBrowRight\",\n",
      "                    \"X\": 0.3558650016784668,\n",
      "                    \"Y\": 0.3780754506587982\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"leftEyeBrowUp\",\n",
      "                    \"X\": 0.40009355545043945,\n",
      "                    \"Y\": 0.38551878929138184\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"rightEyeBrowLeft\",\n",
      "                    \"X\": 0.4924083948135376,\n",
      "                    \"Y\": 0.3846884071826935\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"rightEyeBrowRight\",\n",
      "                    \"X\": 0.5418766140937805,\n",
      "                    \"Y\": 0.37661418318748474\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"rightEyeBrowUp\",\n",
      "                    \"X\": 0.5972506999969482,\n",
      "                    \"Y\": 0.39793750643730164\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"leftEyeLeft\",\n",
      "                    \"X\": 0.3446044623851776,\n",
      "                    \"Y\": 0.4366665184497833\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"leftEyeRight\",\n",
      "                    \"X\": 0.4035806655883789,\n",
      "                    \"Y\": 0.4378509521484375\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"leftEyeUp\",\n",
      "                    \"X\": 0.3702116906642914,\n",
      "                    \"Y\": 0.42829784750938416\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"leftEyeDown\",\n",
      "                    \"X\": 0.3722957968711853,\n",
      "                    \"Y\": 0.4437163770198822\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"rightEyeLeft\",\n",
      "                    \"X\": 0.5012664198875427,\n",
      "                    \"Y\": 0.4372979998588562\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"rightEyeRight\",\n",
      "                    \"X\": 0.5635888576507568,\n",
      "                    \"Y\": 0.4355098605155945\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"rightEyeUp\",\n",
      "                    \"X\": 0.5325345396995544,\n",
      "                    \"Y\": 0.4273603856563568\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"rightEyeDown\",\n",
      "                    \"X\": 0.5323814749717712,\n",
      "                    \"Y\": 0.4428870379924774\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"noseLeft\",\n",
      "                    \"X\": 0.41763246059417725,\n",
      "                    \"Y\": 0.5382756590843201\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"noseRight\",\n",
      "                    \"X\": 0.4771779179573059,\n",
      "                    \"Y\": 0.5379490852355957\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"mouthUp\",\n",
      "                    \"X\": 0.44653695821762085,\n",
      "                    \"Y\": 0.5763041377067566\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"mouthDown\",\n",
      "                    \"X\": 0.4491729736328125,\n",
      "                    \"Y\": 0.6257067918777466\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"leftPupil\",\n",
      "                    \"X\": 0.37144890427589417,\n",
      "                    \"Y\": 0.4370081126689911\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"rightPupil\",\n",
      "                    \"X\": 0.5332980751991272,\n",
      "                    \"Y\": 0.4361833930015564\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"upperJawlineLeft\",\n",
      "                    \"X\": 0.2987395226955414,\n",
      "                    \"Y\": 0.4450984597206116\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"midJawlineLeft\",\n",
      "                    \"X\": 0.3312336504459381,\n",
      "                    \"Y\": 0.6206997036933899\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"chinBottom\",\n",
      "                    \"X\": 0.4565401077270508,\n",
      "                    \"Y\": 0.7124441862106323\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"midJawlineRight\",\n",
      "                    \"X\": 0.6163890361785889,\n",
      "                    \"Y\": 0.6192145347595215\n",
      "                },\n",
      "                {\n",
      "                    \"Type\": \"upperJawlineRight\",\n",
      "                    \"X\": 0.6507094502449036,\n",
      "                    \"Y\": 0.4427793622016907\n",
      "                }\n",
      "            ],\n",
      "            \"Pose\": {\n",
      "                \"Roll\": -2.1811530590057373,\n",
      "                \"Yaw\": -6.1318440437316895,\n",
      "                \"Pitch\": 9.068159103393555\n",
      "            },\n",
      "            \"Quality\": {\n",
      "                \"Brightness\": 97.81294250488281,\n",
      "                \"Sharpness\": 92.22801208496094\n",
      "            },\n",
      "            \"Confidence\": 99.99718475341797\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws rekognition detect-faces \\\n",
    "--image '{\"S3Object\":{\"Bucket\":\"image-api-example\",\"Name\":\"jeremyjacobson.png\"}}' \\\n",
    "--attributes \"ALL\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is much more that can be done, such as detecting text and reading it. See the [linked documentation](https://docs.aws.amazon.com/rekognition/latest/dg/text-detecting-text-procedure.html) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ML Services\n",
    "There are countless new models that are in use today and not offered by AWS as a core ML service. To use them in Sagemaker we can use any of the opensource frameworks for ML with Python such as PyTorch, Tensorflow, or Gluon. The latter is an AWS contributed library which we will focus on in the final example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
